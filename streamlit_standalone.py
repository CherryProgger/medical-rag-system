#!/usr/bin/env python3
"""
–ü–æ–ª–Ω–æ—Å—Ç—å—é –∞–≤—Ç–æ–Ω–æ–º–Ω—ã–π —Ñ–∞–π–ª –¥–ª—è Streamlit Cloud –±–µ–∑ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –æ—Ç –º–æ–¥—É–ª–µ–π
"""

import streamlit as st
import json
import os
import sys
from pathlib import Path
from typing import List, Dict, Any
import numpy as np
import pandas as pd
from sentence_transformers import SentenceTransformer
import faiss
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
from dataclasses import dataclass
import logging

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class Document:
    """–î–æ–∫—É–º–µ–Ω—Ç –¥–ª—è RAG —Å–∏—Å—Ç–µ–º—ã"""
    content: str
    metadata: Dict[str, Any] = None

@dataclass
class Query:
    """–ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
    text: str
    timestamp: str = None

@dataclass
class Response:
    """–û—Ç–≤–µ—Ç RAG —Å–∏—Å—Ç–µ–º—ã"""
    answer: str
    sources: List[str] = None
    confidence: float = 0.0

@dataclass
class Config:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã"""
    embedding_model: str = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
    generation_model: str = "microsoft/DialoGPT-small"  # –ë–æ–ª–µ–µ –ª–µ–≥–∫–∞—è –º–æ–¥–µ–ª—å
    dataset_path: str = "data/rag_clean_dataset_v2_filtered.json"
    max_tokens: int = 128  # –ú–µ–Ω—å—à–µ —Ç–æ–∫–µ–Ω–æ–≤
    top_k: int = 3
    temperature: float = 0.7
    # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–µ –ø–æ–ª—è
    def __post_init__(self):
        pass

class SimpleRAGSystem:
    """–£–ø—Ä–æ—â–µ–Ω–Ω–∞—è RAG —Å–∏—Å—Ç–µ–º–∞ –±–µ–∑ —Å–ª–æ–∂–Ω—ã—Ö –∏–º–ø–æ—Ä—Ç–æ–≤"""
    
    def __init__(self, config: Config):
        self.config = config
        self.embedding_model = None
        self.generation_model = None
        self.tokenizer = None
        self.index = None
        self.documents = []
        self.embeddings = None
        
    def load_models(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª–∏"""
        try:
            logger.info("–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...")
            self.embedding_model = SentenceTransformer(self.config.embedding_model)
            
            logger.info("–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏...")
            self.tokenizer = AutoTokenizer.from_pretrained(self.config.generation_model)
            self.generation_model = AutoModelForCausalLM.from_pretrained(
                self.config.generation_model,
                torch_dtype=torch.float32
            )
            # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –º–æ–¥–µ–ª—å –Ω–∞ CPU
            self.generation_model = self.generation_model.to('cpu')
            
            # –î–æ–±–∞–≤–ª—è–µ–º pad_token –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
                
            logger.info("–ú–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ")
            return True
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–µ–π: {e}")
            return False
    
    def load_data(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ"""
        try:
            data_path = Path(self.config.dataset_path)
            if not data_path.exists():
                logger.error(f"–§–∞–π–ª –¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω: {data_path}")
                return False
                
            with open(data_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            self.documents = []
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞–Ω–Ω—ã—Ö
            if isinstance(data, dict):
                # –ï—Å–ª–∏ —ç—Ç–æ —Å–ª–æ–≤–∞—Ä—å, –∏—â–µ–º –∫–ª—é—á 'data' –∏–ª–∏ 'questions'
                if 'data' in data:
                    items = data['data']
                elif 'questions' in data:
                    items = data['questions']
                else:
                    # –ï—Å–ª–∏ –Ω–µ—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –∫–ª—é—á–µ–π, –±–µ—Ä–µ–º –≤—Å–µ –∑–Ω–∞—á–µ–Ω–∏—è
                    items = list(data.values())
                    # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Å–ø–∏—Å–∫–∏
                    items = [item for item in items if isinstance(item, list)]
                    if items:
                        items = items[0]  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π —Å–ø–∏—Å–æ–∫
                    else:
                        logger.error("–ù–µ –Ω–∞–π–¥–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ –≤ —Å–ª–æ–≤–∞—Ä–µ")
                        return False
            elif isinstance(data, list):
                items = data
            else:
                logger.error(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞–Ω–Ω—ã—Ö: {type(data)}")
                return False
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —ç–ª–µ–º–µ–Ω—Ç—ã
            for item in items:
                if isinstance(item, dict):
                    doc = Document(
                        content=item.get('answer', ''),
                        metadata={'question': item.get('question', '')}
                    )
                    self.documents.append(doc)
                else:
                    logger.warning(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Ç–∏–ø —ç–ª–µ–º–µ–Ω—Ç–∞: {type(item)}")
            
            logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
            return True
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö: {e}")
            return False
    
    def create_embeddings(self):
        """–°–æ–∑–¥–∞–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        try:
            if not self.embedding_model:
                logger.error("–ú–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
                return False
                
            texts = [doc.content for doc in self.documents]
            self.embeddings = self.embedding_model.encode(texts)
            
            # –°–æ–∑–¥–∞–µ–º FAISS –∏–Ω–¥–µ–∫—Å
            dimension = self.embeddings.shape[1]
            self.index = faiss.IndexFlatIP(dimension)
            self.index.add(self.embeddings.astype('float32'))
            
            logger.info(f"–°–æ–∑–¥–∞–Ω–æ {len(self.documents)} —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤")
            return True
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {e}")
            return False
    
    def search_similar(self, query: str, top_k: int = 3) -> List[Document]:
        """–ò—â–µ—Ç –ø–æ—Ö–æ–∂–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã"""
        try:
            if not self.embedding_model or not self.index:
                return []
                
            query_embedding = self.embedding_model.encode([query])
            scores, indices = self.index.search(query_embedding.astype('float32'), top_k)
            
            results = []
            for i, (score, idx) in enumerate(zip(scores[0], indices[0])):
                if idx < len(self.documents):
                    results.append(self.documents[idx])
            
            return results
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
            return []
    
    def generate_answer(self, query: str, context_docs: List[Document]) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
        try:
            # –ü—Ä–æ—Å—Ç–æ–π fallback - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç
            if context_docs:
                # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π –Ω–∞–π–¥–µ–Ω–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –∫–∞–∫ –æ—Ç–≤–µ—Ç
                best_doc = context_docs[0]
                answer = f"–ù–∞ –æ—Å–Ω–æ–≤–µ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏:\n\n{best_doc.content}"
                
                # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
                if len(answer) > 800:
                    answer = answer[:800] + "..."
                
                return answer
            else:
                return "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, –Ω–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–∞—à –≤–æ–ø—Ä–æ—Å."
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
            return "–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å."
    
    def query(self, question: str) -> Response:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
        try:
            # –ò—â–µ–º –ø–æ—Ö–æ–∂–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
            similar_docs = self.search_similar(question, self.config.top_k)
            
            if not similar_docs:
                return Response(
                    answer="–ò–∑–≤–∏–Ω–∏—Ç–µ, –Ω–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–∞—à –≤–æ–ø—Ä–æ—Å.",
                    sources=[],
                    confidence=0.0
                )
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
            answer = self.generate_answer(question, similar_docs)
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏
            sources = [doc.metadata.get('question', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫') for doc in similar_docs]
            
            return Response(
                answer=answer,
                sources=sources,
                confidence=0.8 if similar_docs else 0.0
            )
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–∞: {e}")
            return Response(
                answer=f"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø—Ä–æ—Å–∞: {str(e)}",
                sources=[],
                confidence=0.0
            )

# –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
def load_config():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é"""
    try:
        config_path = Path("config/default.json")
        if config_path.exists():
            with open(config_path, 'r', encoding='utf-8') as f:
                config_data = json.load(f)
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –∏–∑–≤–µ—Å—Ç–Ω—ã–µ –ø–æ–ª—è
            known_fields = {
                'embedding_model', 'generation_model', 'dataset_path', 
                'max_tokens', 'top_k', 'temperature'
            }
            filtered_data = {k: v for k, v in config_data.items() if k in known_fields}
            
            return Config(**filtered_data)
        else:
            return Config()
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {e}")
        return Config()

# –°–æ–∑–¥–∞–µ–º RAG —Å–∏—Å—Ç–µ–º—É
@st.cache_resource
def load_rag_system():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç RAG —Å–∏—Å—Ç–µ–º—É —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
    config = load_config()
    rag_system = SimpleRAGSystem(config)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
    if not rag_system.load_models():
        st.error("–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–µ–π")
        return None
    
    if not rag_system.load_data():
        st.error("–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö")
        return None
    
    if not rag_system.create_embeddings():
        st.error("–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤")
        return None
    
    return rag_system

# –í–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞"""
    try:
        st.set_page_config(
            page_title="Medical RAG System",
            page_icon="üè•",
            layout="wide"
        )
        
        st.title("üè• Medical RAG System")
        st.markdown("–°–∏—Å—Ç–µ–º–∞ –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –ø–æ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º RAG —Å–∏—Å—Ç–µ–º—É
        with st.spinner("–ó–∞–≥—Ä—É–∂–∞–µ–º —Å–∏—Å—Ç–µ–º—É..."):
            rag_system = load_rag_system()
        
        if rag_system is None:
            st.error("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å RAG —Å–∏—Å—Ç–µ–º—É")
            return
        
        st.success("–°–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞ –∫ —Ä–∞–±–æ—Ç–µ!")
        
        # –§–æ—Ä–º–∞ –¥–ª—è –≤–≤–æ–¥–∞ –≤–æ–ø—Ä–æ—Å–∞
        with st.form("question_form"):
            question = st.text_area(
                "–í–≤–µ–¥–∏—Ç–µ –≤–∞—à –≤–æ–ø—Ä–æ—Å:",
                placeholder="–ù–∞–ø—Ä–∏–º–µ—Ä: –ß—Ç–æ —Ç–∞–∫–æ–µ –≤–∞—Ä–∏–∫–æ–∑–Ω–æ–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –≤–µ–Ω?",
                height=100
            )
            submit_button = st.form_submit_button("–ü–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç")
        
        if submit_button and question:
            with st.spinner("–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤–∞—à –≤–æ–ø—Ä–æ—Å..."):
                try:
                    response = rag_system.query(question)
                    
                    st.success("–û—Ç–≤–µ—Ç –ø–æ–ª—É—á–µ–Ω!")
                    st.markdown("### –û—Ç–≤–µ—Ç:")
                    
                    # –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ —Ü–≤–µ—Ç–∞–º–∏
                    answer_text = str(response.answer) if response.answer else "–û—Ç–≤–µ—Ç –Ω–µ –ø–æ–ª—É—á–µ–Ω"
                    st.markdown(f"""
                    <div style='background-color: #2c3e50; padding: 20px; border-radius: 10px; margin: 10px 0; border: 2px solid #3498db;'>
                        <p style='color: #ecf0f1; font-size: 16px; line-height: 1.6; font-weight: 400;'>{answer_text}</p>
                    </div>
                    """, unsafe_allow_html=True)
                    
                    if response.sources and len(response.sources) > 0:
                        st.markdown("### –ò—Å—Ç–æ—á–Ω–∏–∫–∏:")
                        for i, source in enumerate(response.sources, 1):
                            source_text = str(source) if source else f"–ò—Å—Ç–æ—á–Ω–∏–∫ {i}"
                            st.markdown(f"**{i}.** {source_text}")
                            
                except Exception as e:
                    st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–æ–ø—Ä–æ—Å–∞: {str(e)}")
        
        st.markdown("---")
        st.markdown("### –ü—Ä–∏–º–µ—Ä—ã –≤–æ–ø—Ä–æ—Å–æ–≤:")
        st.markdown("- –ß—Ç–æ —Ç–∞–∫–æ–µ –≤–∞—Ä–∏–∫–æ–∑–Ω–æ–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –≤–µ–Ω?")
        st.markdown("- –ö–∞–∫ –ª–µ—á–∏—Ç—å —Ñ–ª–µ–±–∏—Ç—ã?")
        st.markdown("- –ß—Ç–æ —Ç–∞–∫–æ–µ —Ç—Ä–æ–º–±–æ—ç–º–±–æ–ª–∏—è?")
        st.markdown("- –ö–∞–∫–∏–µ —Å–∏–º–ø—Ç–æ–º—ã –∏—à–µ–º–∏—á–µ—Å–∫–æ–≥–æ –∏–Ω—Å—É–ª—å—Ç–∞?")
        
    except Exception as e:
        st.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è: {str(e)}")
        st.markdown("–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É.")

# –ó–∞–ø—É—Å–∫ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
if __name__ == "__main__":
    main()
