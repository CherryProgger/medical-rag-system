#!/usr/bin/env python3
"""
–ü–æ–ª–Ω–æ—Å—Ç—å—é –∞–≤—Ç–æ–Ω–æ–º–Ω—ã–π —Ñ–∞–π–ª –¥–ª—è Streamlit Cloud –±–µ–∑ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –æ—Ç –º–æ–¥—É–ª–µ–π
"""

import streamlit as st
import json
import os
import sys
from pathlib import Path
from typing import List, Dict, Any
import numpy as np
import pandas as pd
from sentence_transformers import SentenceTransformer
import faiss
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
from dataclasses import dataclass
import logging

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class Document:
    """–î–æ–∫—É–º–µ–Ω—Ç –¥–ª—è RAG —Å–∏—Å—Ç–µ–º—ã"""
    content: str
    metadata: Dict[str, Any] = None

@dataclass
class Query:
    """–ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
    text: str
    timestamp: str = None

@dataclass
class Response:
    """–û—Ç–≤–µ—Ç RAG —Å–∏—Å—Ç–µ–º—ã"""
    answer: str
    sources: List[str] = None
    confidence: float = 0.0

@dataclass
class Config:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã"""
    embedding_model: str = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
    generation_model: str = "microsoft/DialoGPT-medium"
    dataset_path: str = "data/rag_clean_dataset_v2_filtered.json"
    max_tokens: int = 256
    top_k: int = 3
    temperature: float = 0.7

class SimpleRAGSystem:
    """–£–ø—Ä–æ—â–µ–Ω–Ω–∞—è RAG —Å–∏—Å—Ç–µ–º–∞ –±–µ–∑ —Å–ª–æ–∂–Ω—ã—Ö –∏–º–ø–æ—Ä—Ç–æ–≤"""
    
    def __init__(self, config: Config):
        self.config = config
        self.embedding_model = None
        self.generation_model = None
        self.tokenizer = None
        self.index = None
        self.documents = []
        self.embeddings = None
        
    def load_models(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª–∏"""
        try:
            logger.info("–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...")
            self.embedding_model = SentenceTransformer(self.config.embedding_model)
            
            logger.info("–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏...")
            self.tokenizer = AutoTokenizer.from_pretrained(self.config.generation_model)
            self.generation_model = AutoModelForCausalLM.from_pretrained(
                self.config.generation_model,
                torch_dtype=torch.float32,
                device_map="cpu"
            )
            
            # –î–æ–±–∞–≤–ª—è–µ–º pad_token –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
                
            logger.info("–ú–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ")
            return True
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–µ–π: {e}")
            return False
    
    def load_data(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ"""
        try:
            data_path = Path(self.config.dataset_path)
            if not data_path.exists():
                logger.error(f"–§–∞–π–ª –¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω: {data_path}")
                return False
                
            with open(data_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            self.documents = []
            for item in data:
                doc = Document(
                    content=item.get('answer', ''),
                    metadata={'question': item.get('question', '')}
                )
                self.documents.append(doc)
            
            logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
            return True
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö: {e}")
            return False
    
    def create_embeddings(self):
        """–°–æ–∑–¥–∞–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        try:
            if not self.embedding_model:
                logger.error("–ú–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
                return False
                
            texts = [doc.content for doc in self.documents]
            self.embeddings = self.embedding_model.encode(texts)
            
            # –°–æ–∑–¥–∞–µ–º FAISS –∏–Ω–¥–µ–∫—Å
            dimension = self.embeddings.shape[1]
            self.index = faiss.IndexFlatIP(dimension)
            self.index.add(self.embeddings.astype('float32'))
            
            logger.info(f"–°–æ–∑–¥–∞–Ω–æ {len(self.documents)} —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤")
            return True
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {e}")
            return False
    
    def search_similar(self, query: str, top_k: int = 3) -> List[Document]:
        """–ò—â–µ—Ç –ø–æ—Ö–æ–∂–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã"""
        try:
            if not self.embedding_model or not self.index:
                return []
                
            query_embedding = self.embedding_model.encode([query])
            scores, indices = self.index.search(query_embedding.astype('float32'), top_k)
            
            results = []
            for i, (score, idx) in enumerate(zip(scores[0], indices[0])):
                if idx < len(self.documents):
                    results.append(self.documents[idx])
            
            return results
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
            return []
    
    def generate_answer(self, query: str, context_docs: List[Document]) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
        try:
            if not self.generation_model or not self.tokenizer:
                return "–ú–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞"
            
            # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            context = "\n".join([doc.content for doc in context_docs[:2]])
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç
            prompt = f"–ö–æ–Ω—Ç–µ–∫—Å—Ç: {context}\n\n–í–æ–ø—Ä–æ—Å: {query}\n\n–û—Ç–≤–µ—Ç:"
            
            # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º
            inputs = self.tokenizer.encode(prompt, return_tensors="pt", max_length=512, truncation=True)
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
            with torch.no_grad():
                outputs = self.generation_model.generate(
                    inputs,
                    max_new_tokens=self.config.max_tokens,
                    temperature=self.config.temperature,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id
                )
            
            # –î–µ–∫–æ–¥–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ –æ—Ç–≤–µ—Ç (–ø–æ—Å–ª–µ "–û—Ç–≤–µ—Ç:")
            if "–û—Ç–≤–µ—Ç:" in response:
                answer = response.split("–û—Ç–≤–µ—Ç:")[-1].strip()
            else:
                answer = response[len(prompt):].strip()
            
            return answer if answer else "–ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç"
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
            return f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞: {str(e)}"
    
    def query(self, question: str) -> Response:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
        try:
            # –ò—â–µ–º –ø–æ—Ö–æ–∂–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
            similar_docs = self.search_similar(question, self.config.top_k)
            
            if not similar_docs:
                return Response(
                    answer="–ò–∑–≤–∏–Ω–∏—Ç–µ, –Ω–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–∞—à –≤–æ–ø—Ä–æ—Å.",
                    sources=[],
                    confidence=0.0
                )
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
            answer = self.generate_answer(question, similar_docs)
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏
            sources = [doc.metadata.get('question', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫') for doc in similar_docs]
            
            return Response(
                answer=answer,
                sources=sources,
                confidence=0.8 if similar_docs else 0.0
            )
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–∞: {e}")
            return Response(
                answer=f"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø—Ä–æ—Å–∞: {str(e)}",
                sources=[],
                confidence=0.0
            )

# –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
def load_config():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é"""
    try:
        config_path = Path("config/default.json")
        if config_path.exists():
            with open(config_path, 'r', encoding='utf-8') as f:
                config_data = json.load(f)
            return Config(**config_data)
        else:
            return Config()
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {e}")
        return Config()

# –°–æ–∑–¥–∞–µ–º RAG —Å–∏—Å—Ç–µ–º—É
@st.cache_resource
def load_rag_system():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç RAG —Å–∏—Å—Ç–µ–º—É —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
    config = load_config()
    rag_system = SimpleRAGSystem(config)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
    if not rag_system.load_models():
        st.error("–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–µ–π")
        return None
    
    if not rag_system.load_data():
        st.error("–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö")
        return None
    
    if not rag_system.create_embeddings():
        st.error("–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤")
        return None
    
    return rag_system

# –í–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞"""
    st.set_page_config(
        page_title="Medical RAG System",
        page_icon="üè•",
        layout="wide"
    )
    
    st.title("üè• Medical RAG System")
    st.markdown("–°–∏—Å—Ç–µ–º–∞ –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –ø–æ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º RAG —Å–∏—Å—Ç–µ–º—É
    with st.spinner("–ó–∞–≥—Ä—É–∂–∞–µ–º —Å–∏—Å—Ç–µ–º—É..."):
        rag_system = load_rag_system()
    
    if rag_system is None:
        st.error("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å RAG —Å–∏—Å—Ç–µ–º—É")
        return
    
    st.success("–°–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞ –∫ —Ä–∞–±–æ—Ç–µ!")
    
    # –§–æ—Ä–º–∞ –¥–ª—è –≤–≤–æ–¥–∞ –≤–æ–ø—Ä–æ—Å–∞
    with st.form("question_form"):
        question = st.text_area(
            "–í–≤–µ–¥–∏—Ç–µ –≤–∞—à –≤–æ–ø—Ä–æ—Å:",
            placeholder="–ù–∞–ø—Ä–∏–º–µ—Ä: –ß—Ç–æ —Ç–∞–∫–æ–µ –≤–∞—Ä–∏–∫–æ–∑–Ω–æ–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –≤–µ–Ω?",
            height=100
        )
        submit_button = st.form_submit_button("–ü–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç")
    
    if submit_button and question:
        with st.spinner("–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤–∞—à –≤–æ–ø—Ä–æ—Å..."):
            try:
                response = rag_system.query(question)
                
                st.success("–û—Ç–≤–µ—Ç –ø–æ–ª—É—á–µ–Ω!")
                st.markdown("### –û—Ç–≤–µ—Ç:")
                st.markdown(f"""
                <div style='background-color: #f0f2f6; padding: 20px; border-radius: 10px; margin: 10px 0;'>
                    <p style='color: #2c3e50; font-size: 16px; line-height: 1.6;'>{response.answer}</p>
                </div>
                """, unsafe_allow_html=True)
                
                if response.sources:
                    st.markdown("### –ò—Å—Ç–æ—á–Ω–∏–∫–∏:")
                    for i, source in enumerate(response.sources, 1):
                        st.markdown(f"**{i}.** {source}")
                        
            except Exception as e:
                st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–æ–ø—Ä–æ—Å–∞: {str(e)}")
    
    st.markdown("---")
    st.markdown("### –ü—Ä–∏–º–µ—Ä—ã –≤–æ–ø—Ä–æ—Å–æ–≤:")
    st.markdown("- –ß—Ç–æ —Ç–∞–∫–æ–µ –≤–∞—Ä–∏–∫–æ–∑–Ω–æ–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –≤–µ–Ω?")
    st.markdown("- –ö–∞–∫ –ª–µ—á–∏—Ç—å —Ñ–ª–µ–±–∏—Ç—ã?")
    st.markdown("- –ß—Ç–æ —Ç–∞–∫–æ–µ —Ç—Ä–æ–º–±–æ—ç–º–±–æ–ª–∏—è?")
    st.markdown("- –ö–∞–∫–∏–µ —Å–∏–º–ø—Ç–æ–º—ã –∏—à–µ–º–∏—á–µ—Å–∫–æ–≥–æ –∏–Ω—Å—É–ª—å—Ç–∞?")

if __name__ == "__main__":
    main()
